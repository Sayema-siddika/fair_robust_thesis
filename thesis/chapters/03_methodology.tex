\chapter{Methodology}
\label{ch:methodology}

This chapter presents our approach to achieving fairness through iterative adaptive sample weighting. We begin with the problem formulation (\S\ref{sec:problem}), then introduce the adaptive weighting mechanism (\S\ref{sec:adaptive_weighting}), describe the iterative training algorithm (\S\ref{sec:algorithm}), analyze the temperature parameter (\S\ref{sec:temperature}), and detail the evaluation metrics and experimental setup (\S\ref{sec:metrics}, \S\ref{sec:setup}).

\section{Problem Formulation}
\label{sec:problem}

We consider a standard supervised binary classification setting with sensitive attributes. Let $\mathcal{D} = \{(x_i, y_i, z_i)\}_{i=1}^n$ denote the training dataset, where:
\begin{itemize}
    \item $x_i \in \mathbb{R}^d$ is the feature vector for sample $i$
    \item $y_i \in \{0,1\}$ is the true label
    \item $z_i \in \{0,1\}$ is the sensitive attribute (e.g., gender, race)
\end{itemize}

We aim to learn a classifier $f_\theta: \mathbb{R}^d \to [0,1]$ parametrized by $\theta$ that produces calibrated probability estimates $\hat{y}_i = f_\theta(x_i)$. Binary predictions are obtained by thresholding: $\tilde{y}_i = \mathbb{1}[\hat{y}_i \geq 0.5]$.

\subsection{Fairness Objective}

Our primary objective is to achieve \textbf{equalized odds} (EO)~\cite{hardt2016equality}, which requires the classifier to have equal true positive rates and false positive rates across sensitive groups:
\begin{align}
    \text{TPR}_0 &= \text{TPR}_1 \label{eq:eo_tpr} \\
    \text{FPR}_0 &= \text{FPR}_1 \label{eq:eo_fpr}
\end{align}
where $\text{TPR}_z = P(\tilde{Y}=1 | Y=1, Z=z)$ and $\text{FPR}_z = P(\tilde{Y}=1 | Y=0, Z=z)$.

We measure EO violation as:
\begin{equation}
    \text{EO} = \max\left( |\text{TPR}_0 - \text{TPR}_1|, |\text{FPR}_0 - \text{FPR}_1| \right)
\end{equation}

Perfect fairness corresponds to $\text{EO} = 0$. We also report demographic parity (DP) violations:
\begin{equation}
    \text{DP} = |P(\tilde{Y}=1 | Z=0) - P(\tilde{Y}=1 | Z=1)|
\end{equation}

\subsection{Trade-offs}

As discussed in Chapter~\ref{ch:related_work}, achieving perfect fairness may come at the cost of:
\begin{itemize}
    \item \textbf{Accuracy}: Overall classification performance may degrade
    \item \textbf{Calibration}: Probability estimates may become miscalibrated
    \item \textbf{Computational cost}: Training time may increase significantly
\end{itemize}

Our methodology explicitly tracks these trade-offs to provide practitioners with deployment guidance.

\section{Adaptive Sample Weighting}
\label{sec:adaptive_weighting}

The core innovation of our approach is an \emph{iterative adaptive weighting} mechanism that assigns training weights to samples based on both their \textbf{confidence} and \textbf{correctness} in the current model.

\subsection{Weight Formula}

Given a trained model $f_\theta$ at iteration $t$, we compute sample weights $w_i^{(t)}$ as:
\begin{equation}
    w_i^{(t)} = \left( c_i^{(t)} \times r_i^{(t)} + \epsilon \right)^{1/T}
    \label{eq:weight_formula}
\end{equation}
where:
\begin{itemize}
    \item $c_i^{(t)} = |f_\theta(x_i) - 0.5|$ is the \textbf{confidence}: distance from decision boundary
    \item $r_i^{(t)} = \mathbb{1}[\text{prediction correct}]$ is the \textbf{correctness} indicator
    \item $\epsilon = 10^{-8}$ is a small constant for numerical stability
    \item $T > 0$ is a temperature parameter controlling weight concentration
\end{itemize}

\subsection{Mechanism Intuition}

This weighting scheme has a counterintuitive property: it \emph{upweights samples the model already predicts correctly with high confidence}, rather than focusing on difficult misclassified examples (as in boosting~\cite{freund1997decision}).

\textbf{Why does this improve fairness?} Our empirical analysis (Chapter~\ref{ch:results}) reveals the mechanism:
\begin{enumerate}
    \item Samples from the \textbf{disadvantaged group} tend to have \emph{lower average confidence}, even when correctly classified
    \item The formula $w_i = (c_i \times r_i + \epsilon)^{1/T}$ amplifies small differences in confidence via the exponent
    \item This selectively boosts the disadvantaged group's correct predictions, rebalancing group-wise performance
    \item Over iterations, this drives TPR and FPR toward parity across groups
\end{enumerate}

\subsection{Comparison to Existing Methods}

Our weighting differs fundamentally from prior approaches:
\begin{itemize}
    \item \textbf{vs. Boosting}~\cite{freund1997decision}: Boosting upweights \emph{misclassified} samples; we upweight \emph{correct, confident} samples
    \item \textbf{vs. Cost-sensitive learning}~\cite{elkan2001foundations}: Fixed costs per group; we adapt weights \emph{dynamically} based on model state
    \item \textbf{vs. Importance weighting}~\cite{shimodaira2000improving}: Corrects distribution shift; we target \emph{fairness} objectives
    \item \textbf{vs. Fairness-aware reweighting}~\cite{calders2009building, lahoti2020fairness}: Prior methods use group membership or adversarial objectives; we use \emph{confidence $\times$ correctness}
\end{itemize}

\section{Iterative Training Algorithm}
\label{sec:algorithm}

Algorithm~\ref{alg:adaptive} presents the complete iterative training procedure.

\begin{algorithm}[t]
\caption{Iterative Adaptive Sample Weighting for Fairness}
\label{alg:adaptive}
\begin{algorithmic}[1]
\REQUIRE Training data $\mathcal{D} = \{(x_i, y_i, z_i)\}_{i=1}^n$, temperature $T$, iterations $K$, model class $\mathcal{M}$
\ENSURE Fair classifier $f_\theta$
\STATE Initialize weights: $w_i^{(0)} = 1$ for all $i \in [n]$
\FOR{$t = 1$ to $K$}
    \STATE \textcolor{blue}{// Train model with current weights}
    \STATE $\theta^{(t)} \gets \arg\min_\theta \sum_{i=1}^n w_i^{(t-1)} \cdot \ell(f_\theta(x_i), y_i)$
    \STATE
    \STATE \textcolor{blue}{// Compute predictions}
    \STATE $\hat{y}_i^{(t)} \gets f_{\theta^{(t)}}(x_i)$ for all $i$
    \STATE $\tilde{y}_i^{(t)} \gets \mathbb{1}[\hat{y}_i^{(t)} \geq 0.5]$ for all $i$
    \STATE
    \STATE \textcolor{blue}{// Compute adaptive weights}
    \FOR{$i = 1$ to $n$}
        \STATE $c_i^{(t)} \gets |\hat{y}_i^{(t)} - 0.5|$ \hfill \textcolor{gray}{// Confidence}
        \STATE $r_i^{(t)} \gets \mathbb{1}[\tilde{y}_i^{(t)} = y_i]$ \hfill \textcolor{gray}{// Correctness}
        \STATE $w_i^{(t)} \gets (c_i^{(t)} \times r_i^{(t)} + \epsilon)^{1/T}$
    \ENDFOR
    \STATE
    \STATE \textcolor{blue}{// Evaluate fairness}
    \STATE Compute $\text{EO}^{(t)}$, $\text{DP}^{(t)}$ on validation set
    \IF{$\text{EO}^{(t)} < \delta$} \hfill \textcolor{gray}{// Fairness threshold}
        \STATE \textbf{break} \hfill \textcolor{gray}{// Early stopping}
    \ENDIF
\ENDFOR
\RETURN $f_{\theta^{(K)}}$
\end{algorithmic}
\end{algorithm}

\subsection{Key Steps}

\begin{enumerate}
    \item \textbf{Initialization} (line 1): All samples start with equal weight $w_i = 1$
    \item \textbf{Weighted training} (line 3): Train model to minimize weighted loss:
    \begin{equation}
        \mathcal{L}_{\text{weighted}}(\theta) = \sum_{i=1}^n w_i \cdot \ell(f_\theta(x_i), y_i)
    \end{equation}
    where $\ell$ is binary cross-entropy loss
    \item \textbf{Weight update} (lines 9-11): Recompute weights based on current model's confidence and correctness
    \item \textbf{Early stopping} (lines 13-15): Terminate if fairness threshold $\delta$ is achieved (we use $\delta = 0.01$)
\end{enumerate}

\subsection{Computational Complexity}

Each iteration requires:
\begin{itemize}
    \item \textbf{Training}: $O(n \cdot d \cdot E)$ where $E$ is the cost of one training epoch
    \item \textbf{Weight computation}: $O(n)$ for computing $c_i$, $r_i$, $w_i$
    \item \textbf{Evaluation}: $O(n)$ for fairness metrics
\end{itemize}

\textbf{Total cost}: $O(K \cdot n \cdot d \cdot E)$, where $K$ is typically small (3-10 iterations). The per-iteration overhead of weight computation is negligible compared to training.

\subsection{Implementation Details}

Our implementation uses:
\begin{itemize}
    \item \textbf{Model class}: Logistic regression (scikit-learn \texttt{LogisticRegression})
    \item \textbf{Solver}: L-BFGS with L2 regularization ($C=1.0$)
    \item \textbf{Maximum iterations}: 1000 per training step
    \item \textbf{Convergence tolerance}: $10^{-4}$
\end{itemize}

Weights are passed via scikit-learn's \texttt{sample\_weight} parameter, which multiplies the loss for each sample.

\section{Temperature Parameter Analysis}
\label{sec:temperature}

The temperature $T$ controls the \emph{concentration} of weights through the exponent $1/T$ in Equation~\ref{eq:weight_formula}.

\subsection{Effect of Temperature}

\begin{itemize}
    \item \textbf{Low $T$ ($T \ll 1$)}: Exponent $1/T$ is large $\Rightarrow$ weights become highly concentrated on high-confidence correct predictions. This creates \emph{strong} reweighting but may lead to:
    \begin{itemize}
        \item Overfitting to specific samples
        \item Extreme weight distributions (some $w_i \gg 1$)
        \item Numerical instability
    \end{itemize}
    
    \item \textbf{High $T$ ($T \gg 1$)}: Exponent $1/T$ is small $\Rightarrow$ weights remain closer to uniform. This creates \emph{gentle} reweighting:
    \begin{itemize}
        \item More stable training
        \item Slower fairness improvement
        \item May fail to achieve perfect fairness
    \end{itemize}
    
    \item \textbf{Optimal $T$}: We empirically find $T \in [0.5, 2.0]$ balances fairness improvement with stability across datasets
\end{itemize}

\subsection{Temperature Sweep Experiments}

In Chapter~\ref{ch:results}, we report results from sweeping $T \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$ on all datasets to characterize:
\begin{enumerate}
    \item Best fairness achieved for each $T$
    \item Calibration degradation vs. $T$
    \item Training time vs. $T$
\end{enumerate}

\section{Evaluation Metrics}
\label{sec:metrics}

We evaluate models across four dimensions: fairness, accuracy, calibration, and efficiency.

\subsection{Fairness Metrics}

\textbf{Equalized Odds (EO)}:
\begin{equation}
    \text{EO} = \max\left( |\text{TPR}_0 - \text{TPR}_1|, |\text{FPR}_0 - \text{FPR}_1| \right)
\end{equation}

\textbf{Demographic Parity (DP)}:
\begin{equation}
    \text{DP} = |P(\tilde{Y}=1 | Z=0) - P(\tilde{Y}=1 | Z=1)|
\end{equation}

\subsection{Accuracy Metrics}

\textbf{Accuracy}: Fraction of correct predictions
\begin{equation}
    \text{Accuracy} = \frac{1}{n} \sum_{i=1}^n \mathbb{1}[\tilde{y}_i = y_i]
\end{equation}

\textbf{Balanced Accuracy}: Average of per-class accuracies (robust to class imbalance)
\begin{equation}
    \text{Balanced Acc} = \frac{1}{2} \left( \text{TPR} + \text{TNR} \right)
\end{equation}

\subsection{Calibration Metrics}

\textbf{Expected Calibration Error (ECE)}~\cite{guo2017calibration}: Measures average difference between confidence and accuracy across bins
\begin{equation}
    \text{ECE} = \sum_{b=1}^B \frac{|B_b|}{n} |\text{acc}(B_b) - \text{conf}(B_b)|
\end{equation}
where $B_b$ are bins of predictions (we use 10 bins), $\text{acc}(B_b)$ is accuracy in bin $b$, and $\text{conf}(B_b)$ is average confidence in bin $b$.

\textbf{Brier Score}~\cite{brier1950verification}: Mean squared error of probability predictions
\begin{equation}
    \text{Brier} = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2
\end{equation}

Lower ECE and Brier indicate better calibration.

\subsection{Efficiency Metrics}

\textbf{Training Time}: Wall-clock time for iterative training (includes weight computation overhead)

\textbf{Iterations to Convergence}: Number of iterations $K$ required to achieve $\text{EO} < 0.01$

\textbf{Inference Time}: Per-sample prediction time (should be identical to baseline since no test-time modifications)

\section{Experimental Setup}
\label{sec:setup}

\subsection{Datasets}

We evaluate on three standard fairness benchmark datasets:

\textbf{Adult Income}~\cite{bellamy2018ai}: Predict income $>$50K from census data
\begin{itemize}
    \item Samples: 45,222 (train/test split)
    \item Features: 14 (age, education, occupation, etc.)
    \item Sensitive attribute: Gender (0=Female, 1=Male)
    \item Base rate: 24.1\% positive class
\end{itemize}

\textbf{COMPAS Recidivism}~\cite{angwin2016machine}: Predict recidivism risk
\begin{itemize}
    \item Samples: 6,172 (train/test split)
    \item Features: 11 (age, priors, charge degree, etc.)
    \item Sensitive attribute: Race (0=Caucasian, 1=African-American)
    \item Base rate: 45.6\% positive class
\end{itemize}

\textbf{German Credit}~\cite{bellamy2018ai}: Predict credit risk
\begin{itemize}
    \item Samples: 1,000 (5-fold CV)
    \item Features: 20 (account status, credit history, etc.)
    \item Sensitive attribute: Age (0=$<$25, 1=$\geq$25)
    \item Base rate: 30.0\% positive class
\end{itemize}

All datasets are preprocessed using the AIF360 toolkit~\cite{bellamy2018ai} with standard train/test splits (70/30) and 5-fold cross-validation for German Credit.

\subsection{Baseline Methods}

We compare against:

\textbf{Unweighted}: Standard logistic regression (no fairness intervention)

\textbf{Reweighing}~\cite{kamiran2012data}: Pre-processing method that assigns group-based weights to achieve demographic parity

\textbf{Prejudice Remover}~\cite{kamiran2012data}: In-processing regularization adding fairness penalty to loss

\textbf{Calibrated Equalized Odds}~\cite{pleiss2017fairness}: Post-processing method that adjusts thresholds per group

\textbf{Meta-Learning + Adaptive (Hybrid)}: Our Day 15 experiments showed pure adaptive ($\alpha=0$) outperforms hybrid meta-learning approaches, so we focus on the pure adaptive method

\subsection{Hyperparameters}

\textbf{Temperature sweep}: $T \in \{0.1, 0.5, 1.0, 2.0, 5.0\}$

\textbf{Iterations}: $K = 10$ (with early stopping if $\text{EO} < 0.01$)

\textbf{Fairness threshold}: $\delta = 0.01$ for early stopping

\textbf{Regularization}: $C = 1.0$ (inverse regularization strength)

\textbf{Random seed}: 42 for reproducibility

\subsection{Computational Environment}

\textbf{Hardware}: AMD Ryzen 7 / Intel Core i7 CPU (no GPU required)

\textbf{Software}: Python 3.8, scikit-learn 1.3.0, AIF360 0.5.0, NumPy 1.24.3

\textbf{Parallelization}: None (single-threaded training)

\subsection{Evaluation Protocol}

For each dataset and temperature $T$:
\begin{enumerate}
    \item Train iterative adaptive weighting (Algorithm~\ref{alg:adaptive})
    \item Train all baseline methods
    \item Evaluate all methods on held-out test set
    \item Record fairness (EO, DP), accuracy, calibration (ECE, Brier), and training time
    \item Repeat for 5 random seeds and report mean $\pm$ std
\end{enumerate}

For German Credit, we use 5-fold cross-validation due to small sample size.

\subsection{Statistical Significance}

We use paired t-tests to assess whether differences in metrics are statistically significant ($p < 0.05$). Results are marked with * for $p < 0.05$ and ** for $p < 0.01$.

\section{Summary}

This chapter introduced our iterative adaptive sample weighting methodology for achieving fairness in binary classification. The key innovations are:
\begin{enumerate}
    \item \textbf{Counterintuitive weighting}: Upweighting confident correct predictions (not misclassified samples)
    \item \textbf{Iterative refinement}: Recomputing weights based on evolving model state
    \item \textbf{Temperature control}: Balancing fairness improvement with stability via $T$
    \item \textbf{Computational efficiency}: Negligible overhead for weight computation, no inference changes
\end{enumerate}

The next chapter presents comprehensive experimental results evaluating this approach across three datasets, comparing against four baseline methods, and analyzing the fundamental trade-offs between fairness, accuracy, and calibration.
