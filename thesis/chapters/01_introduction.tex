\chapter{Introduction}
\label{ch:introduction}

\section{Motivation}
\label{sec:motivation}

Machine learning systems have become ubiquitous in modern society, influencing decisions that profoundly affect individuals' lives. From determining who receives a loan~\cite{obermeyer2019dissecting} to predicting recidivism risk in criminal justice~\cite{angwin2016machine}, these automated systems increasingly replace or augment human decision-making. However, numerous studies have documented that ML models can perpetuate and even amplify societal biases, leading to discriminatory outcomes against protected demographic groups~\cite{barocas2016big,mehrabi2021survey}.

The consequences of unfair algorithms extend beyond individual harm. In 2016, ProPublica's investigation of the COMPAS recidivism prediction system revealed that Black defendants were nearly twice as likely to be falsely labeled as high-risk compared to white defendants~\cite{angwin2016machine}. Similarly, Amazon's recruiting tool showed systematic bias against women, automatically downranking resumes containing the word ``women's''~\cite{dastin2018amazon}. These high-profile cases demonstrate that without careful intervention, ML systems can encode and perpetuate historical inequities present in training data.

The need for fair machine learning has never been more urgent. As governments worldwide introduce regulations mandating algorithmic accountability---such as the EU's General Data Protection Regulation (GDPR)~\cite{voigt2017eu} and proposed AI Acts~\cite{veale2021demystifying}---organizations face both ethical imperatives and legal requirements to ensure their ML systems treat all demographic groups equitably. However, achieving fairness in practice remains challenging due to three fundamental obstacles:

\begin{enumerate}
    \item \textbf{Incomplete fairness improvements}: Most existing methods reduce but do not eliminate disparities. Post-processing approaches~\cite{hardt2016equality} modify predictions after training but achieve limited fairness gains. Constrained optimization methods~\cite{zafar2017fairness} impose fairness constraints during training but struggle with convergence and computational complexity.
    
    \item \textbf{Unclear trade-offs}: The relationship between fairness and other desiderata---accuracy, calibration, computational cost---remains poorly understood. Prior work often reports fairness improvements in isolation without systematically characterizing what is sacrificed to achieve them~\cite{corbett2018measure}.
    
    \item \textbf{Deployment barriers}: Many fairness interventions introduce production overhead (post-processing requires per-prediction adjustments) or require extensive reengineering of existing ML pipelines (adversarial debiasing~\cite{zhang2018mitigating} necessitates custom training procedures).
\end{enumerate}

This thesis addresses these challenges through \textbf{iterative adaptive sample weighting}, a simple in-processing method that: (1)~achieves perfect fairness (equalized odds disparity EO=0.0) on real-world datasets, (2)~comprehensively characterizes the fairness-calibration trade-off, and (3)~requires no deployment overhead, producing standard models compatible with existing infrastructure.

\section{Research Questions}
\label{sec:research_questions}

This thesis investigates four interconnected research questions:

\begin{description}
    \item[\textbf{RQ1: Fairness Achievement}] Can adaptive sample weighting achieve perfect fairness on real-world datasets?
    
    Prior work has demonstrated fairness improvements through sample weighting~\cite{kamiran2012data}, but no method has achieved perfect equity (EO=0.0, DP=0.0) on standard benchmarks. We hypothesize that iterative refinement of sample weights, informed by model confidence and prediction correctness, can progressively reduce disparities to zero.
    
    \item[\textbf{RQ2: Trade-off Characterization}] What are the fundamental trade-offs between fairness and other performance metrics?
    
    The fairness-accuracy trade-off has been studied extensively~\cite{menon2018cost}, but the fairness-calibration relationship remains underexplored. Calibration---the alignment between predicted probabilities and true frequencies~\cite{guo2017calibration}---is critical for applications requiring probability interpretations (e.g., medical diagnosis). We systematically measure how fairness improvements affect calibration quality.
    
    \item[\textbf{RQ3: Computational Feasibility}] Is adaptive weighting computationally viable for production deployment?
    
    Methods requiring significant training overhead (e.g., meta-learning~\cite{finn2017model}) or inference-time adjustments (e.g., post-processing~\cite{hardt2016equality}) face adoption barriers. We quantify training time, memory usage, inference latency, and scalability to assess real-world deployability.
    
    \item[\textbf{RQ4: Mechanism Understanding}] Why does adaptive weighting improve fairness?
    
    Black-box fairness improvements offer limited actionable insights. Through interpretability analysis---examining weight distributions, feature importance changes, and high-weight sample characteristics---we aim to understand \emph{why} the method works, enabling practitioners to predict when it will succeed or fail.
\end{description}

\section{Contributions}
\label{sec:contributions}

This thesis makes six key contributions to the fair machine learning literature:

\subsection{Novel Method: Iterative Adaptive Weighting}

We introduce a simple yet effective fairness intervention based on iterative sample reweighting. The core idea is to assign higher weights to samples the model predicts correctly with high confidence:

\begin{equation}
    w_i = \left( c_i \times r_i + \epsilon \right)^{1/T}
\end{equation}

where $c_i = \max(p_i, 1-p_i)$ is prediction confidence, $r_i \in \{0,1\}$ indicates correctness, $\epsilon=0.1$ provides stability, and $T=0.5$ is the temperature parameter. Unlike prior weighting schemes that target misclassified samples~\cite{freund1997decision}, our approach reinforces confident correct predictions, counterintuitively improving fairness by focusing on ``easy'' samples the model already understands.

The method requires no fairness-specific constraints, adversarial training, or post-processing---just standard weighted logistic regression repeated for 10--20 iterations. This simplicity facilitates adoption in production systems.

\subsection{Perfect Fairness on Real Data}

We demonstrate, for the first time, \textbf{perfect equalized odds (EO=0.0) and demographic parity (DP=0.0)} on the German Credit dataset, a standard fairness benchmark. Previous work has achieved fairness improvements~\cite{kamiran2012data,zafar2017fairness} but never complete elimination of disparities on real datasets.

On the Adult Income dataset, we achieve +30.9\% fairness improvement, reducing EO from 0.0518 to 0.0358. These results establish that perfect fairness is \emph{achievable} in practice, not merely a theoretical ideal.

\subsection{Fairness-Calibration Trade-off Quantification}

Through systematic calibration analysis across three datasets, we discover a \textbf{fundamental trade-off}: fairness improvements universally degrade calibration by +388--756\% (Expected Calibration Error, ECE). This finding challenges the assumption that fairness interventions primarily trade off accuracy; instead, \emph{calibration} is the primary casualty.

We characterize the trade-off mechanism: adaptive weighting upweights confident correct predictions, causing the model to focus on ``easy'' regions while ignoring uncertain boundaries. This creates overconfident predictions---the model becomes surer of its decisions but less calibrated.

This negative result is a significant contribution. Knowing when methods fail is as valuable as knowing when they succeed~\cite{sculley2018winner}.

\subsection{Mechanism Interpretability}

Unlike black-box fairness methods, we provide transparency into \emph{why} adaptive weighting works through three analyses:

\begin{enumerate}
    \item \textbf{Weight distribution analysis}: High-weight samples are predominantly confident correct predictions; on German Credit, 100\% of high-weight samples are correctly classified negatives.
    
    \item \textbf{Coefficient change analysis}: Model coefficients shift dramatically (increases of +340--5666\%) under adaptive weighting, indicating fundamental changes in feature reliance.
    
    \item \textbf{Feature correlation analysis}: Negative correlations between weights and features (e.g., education $r=-0.40$, age $r=-0.37$ on Adult) reveal that younger, less educated samples---counterintuitively---receive higher weights because they are ``easier'' to classify correctly.
\end{enumerate}

This interpretability enables practitioners to predict when adaptive weighting will succeed (datasets with clear ``easy'' samples) and when it will fail (datasets already near-optimal).

\subsection{Computational Efficiency Characterization}

We provide comprehensive efficiency analysis showing adaptive weighting is production-viable:

\begin{itemize}
    \item \textbf{Training time}: 0.05--1.5 seconds (12--22$\times$ baseline overhead), acceptable for offline training
    \item \textbf{Memory usage}: <10 MB peak, negligible for modern systems
    \item \textbf{Inference time}: \textbf{Zero overhead}---adaptive models are standard logistic regression classifiers
    \item \textbf{Scalability}: Linear $O(n)$ scaling confirmed on datasets from 1K to 30K samples
\end{itemize}

The zero inference overhead is particularly significant. Unlike post-processing methods that adjust predictions at inference time~\cite{hardt2016equality}, our approach produces models indistinguishable from standard classifiers in production.

\subsection{Practical Deployment Guidelines}

Beyond empirical results, we provide actionable recommendations for practitioners:

\begin{itemize}
    \item \textbf{When to use}: High baseline unfairness (EO $>$ 0.10), fairness priority, calibration less critical
    \item \textbf{When to avoid}: Already-fair baselines (EO $<$ 0.05), calibration-critical applications (medical diagnosis, finance), real-time training
    \item \textbf{Configuration}: Temperature $T=0.5$, iterations $K=10$--15, early stopping when fairness improvement plateaus
    \item \textbf{Monitoring}: Track both fairness (EO, DP) \emph{and} calibration (ECE) in production; retrain when either degrades
\end{itemize}

These guidelines transform theoretical contributions into practical tools for building fairer ML systems.

\section{Thesis Outline}
\label{sec:outline}

The remainder of this thesis is organized as follows:

\begin{description}
    \item[\textbf{Chapter~\ref{ch:related_work}: Related Work}] surveys fairness definitions, existing fairness interventions (pre-processing, in-processing, post-processing), calibration concepts, and sample weighting methods. We position our contributions relative to prior work.
    
    \item[\textbf{Chapter~\ref{ch:methodology}: Methodology}] formalizes the adaptive sample weighting approach, including weight computation, iterative training algorithm, temperature parameter selection, and evaluation metrics (equalized odds, calibration error).
    
    \item[\textbf{Chapter~\ref{ch:experiments}: Experimental Setup}] describes datasets (COMPAS, Adult, German), preprocessing, implementation details, and experimental protocol covering 21 days of systematic investigation.
    
    \item[\textbf{Chapter~\ref{ch:results}: Results}] presents empirical findings: fairness improvements (perfect EO=0.0 on German, +30.9\% on Adult), calibration degradation (+388--756\% ECE), interpretability analysis (coefficient changes, feature correlations), and computational efficiency (<2s training, zero inference overhead).
    
    \item[\textbf{Chapter~\ref{ch:discussion}: Discussion}] interprets results, answers research questions, provides deployment guidelines, acknowledges limitations (calibration trade-off, dataset dependence), and compares to existing methods.
    
    \item[\textbf{Chapter~\ref{ch:conclusion}: Conclusion}] summarizes contributions, discusses broader impact, and proposes future work (calibration-preserving fairness, neural network extension, theoretical analysis).
\end{description}

\section{Notation and Definitions}
\label{sec:notation}

We establish notation used throughout this thesis:

\subsection{Data and Model}

\begin{itemize}
    \item $\mathcal{D} = \{(x_i, y_i, z_i)\}_{i=1}^n$: Dataset with $n$ samples
    \item $x_i \in \mathbb{R}^d$: Feature vector for sample $i$ ($d$ features)
    \item $y_i \in \{0, 1\}$: Binary label (0 = negative, 1 = positive)
    \item $z_i \in \{0, 1\}$: Sensitive attribute (0 = majority, 1 = protected group)
    \item $f_\theta: \mathbb{R}^d \to [0,1]$: Model parameterized by $\theta$
    \item $\hat{y}_i = \mathbb{1}[f_\theta(x_i) \geq 0.5]$: Predicted label
    \item $p_i = f_\theta(x_i)$: Predicted probability
\end{itemize}

\subsection{Fairness Metrics}

\begin{definition}[Equalized Odds Disparity]
    The equalized odds disparity measures the maximum difference in true positive rates (TPR) and false positive rates (FPR) between demographic groups:
    \begin{equation}
        \text{EO} = \max\left( |\text{TPR}_0 - \text{TPR}_1|, |\text{FPR}_0 - \text{FPR}_1| \right)
    \end{equation}
    where $\text{TPR}_z = P(\hat{Y}=1 \mid Y=1, Z=z)$ and $\text{FPR}_z = P(\hat{Y}=1 \mid Y=0, Z=z)$. Perfect fairness corresponds to EO = 0.
\end{definition}

\begin{definition}[Demographic Parity Disparity]
    Demographic parity measures the difference in positive prediction rates between groups:
    \begin{equation}
        \text{DP} = |P(\hat{Y}=1 \mid Z=0) - P(\hat{Y}=1 \mid Z=1)|
    \end{equation}
    Perfect parity corresponds to DP = 0.
\end{definition}

\subsection{Calibration Metrics}

\begin{definition}[Expected Calibration Error]
    Expected Calibration Error (ECE) measures miscalibration by partitioning predictions into $B$ bins and computing:
    \begin{equation}
        \text{ECE} = \sum_{b=1}^B \frac{n_b}{n} \left| \text{acc}(b) - \text{conf}(b) \right|
    \end{equation}
    where $n_b$ is the number of samples in bin $b$, acc$(b)$ is the accuracy in bin $b$, and conf$(b)$ is the average confidence in bin $b$. Perfect calibration corresponds to ECE = 0.
\end{definition}

\begin{definition}[Brier Score]
    The Brier score measures mean squared error of probability predictions:
    \begin{equation}
        \text{Brier} = \frac{1}{n} \sum_{i=1}^n (p_i - y_i)^2
    \end{equation}
    Lower is better; perfectly calibrated predictions achieve minimal Brier score.
\end{definition}

\subsection{Sample Weighting}

\begin{itemize}
    \item $w_i \in \mathbb{R}^+$: Weight for sample $i$
    \item $c_i = \max(p_i, 1-p_i)$: Prediction confidence ($c_i \in [0.5, 1]$)
    \item $r_i = \mathbb{1}[\hat{y}_i = y_i]$: Correctness indicator
    \item $T > 0$: Temperature parameter controlling weight sharpness
    \item $\epsilon > 0$: Stability constant (prevents zero weights)
\end{itemize}

The adaptive weighting formula is:
\begin{equation}
    w_i = \left( c_i \times r_i + \epsilon \right)^{1/T}
\end{equation}

With this notation established, we proceed to review related work in fairness, calibration, and sample weighting.
