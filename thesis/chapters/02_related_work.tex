\chapter{Related Work}
\label{ch:related_work}

This chapter surveys prior work in fair machine learning, calibration, and sample weighting. We organize the discussion into five sections: fairness definitions and impossibility results (\S\ref{sec:fairness_defs}), fairness interventions (\S\ref{sec:fairness_interventions}), meta-learning for fairness (\S\ref{sec:meta_fairness}), calibration (\S\ref{sec:calibration}), and sample weighting (\S\ref{sec:sample_weighting}).

\section{Fairness Definitions}
\label{sec:fairness_defs}

The fairness literature has proposed numerous mathematical definitions of equity, broadly categorized into individual and group fairness.

\subsection{Individual Fairness}

\textbf{Individual fairness}~\cite{dwork2012fairness} requires that similar individuals receive similar treatment. Formally, a classifier $f$ satisfies individual fairness if there exists a task-specific distance metric $d$ such that for all $x, x'$:
\begin{equation}
    d(x, x') \leq \epsilon \implies |f(x) - f(x')| \leq \delta
\end{equation}

While philosophically appealing, individual fairness faces two practical challenges: (1)~defining an appropriate similarity metric $d$ is domain-dependent and requires expert knowledge, and (2)~verifying fairness requires pairwise comparisons, computationally infeasible for large datasets.

\subsection{Group Fairness}

\textbf{Group fairness} definitions require statistical parity across demographic groups defined by sensitive attributes (e.g., race, gender, age). The three most prominent notions are:

\paragraph{Demographic Parity.}
Also called statistical parity~\cite{calders2009building}, demographic parity requires equal positive prediction rates:
\begin{equation}
    P(\hat{Y}=1 \mid Z=0) = P(\hat{Y}=1 \mid Z=1)
\end{equation}

This definition is independence-based: predictions should be independent of the sensitive attribute. However, demographic parity can conflict with accuracy when base rates differ between groups~\cite{chouldechova2017fair}.

\paragraph{Equalized Odds.}
Introduced by Hardt et al.~\cite{hardt2016equality}, equalized odds requires equal true positive and false positive rates:
\begin{align}
    P(\hat{Y}=1 \mid Y=y, Z=0) &= P(\hat{Y}=1 \mid Y=y, Z=1) \quad \forall y \in \{0,1\}
\end{align}

This definition is separation-based: predictions should be independent of the sensitive attribute conditional on the true label. Equalized odds allows different positive rates when justified by different base rates.

\paragraph{Equal Opportunity.}
A relaxation of equalized odds~\cite{hardt2016equality}, equal opportunity requires only equal true positive rates:
\begin{equation}
    P(\hat{Y}=1 \mid Y=1, Z=0) = P(\hat{Y}=1 \mid Y=1, Z=1)
\end{equation}

This definition prioritizes fairness for the positive class, appropriate when false positives are less harmful than false negatives (e.g., loan approvals).

\subsection{Impossibility Results}

Multiple impossibility theorems show that fairness definitions can conflict:

\begin{theorem}[Chouldechova~\cite{chouldechova2017fair}]
    Except in degenerate cases, a classifier cannot simultaneously satisfy calibration, balance for the negative class, and balance for the positive class when base rates differ between groups.
\end{theorem}

\begin{theorem}[Kleinberg et al.~\cite{kleinberg2016inherent}]
    Except when base rates are equal or the classifier is perfect, no classifier can simultaneously satisfy calibration, balance for the positive class, and balance for the negative class.
\end{theorem}

These results imply trade-offs are unavoidable. Our work quantifies one such trade-off: fairness (equalized odds) versus calibration quality.

\section{Fairness Interventions}
\label{sec:fairness_interventions}

Fairness interventions are typically categorized by when they are applied: before training (pre-processing), during training (in-processing), or after training (post-processing).

\subsection{Pre-processing Methods}

Pre-processing approaches modify training data to remove bias before model training.

\paragraph{Data Reweighing.}
Kamiran and Calders~\cite{kamiran2012data} assign weights to training samples to balance positive and negative examples across groups. Samples from underrepresented group-label combinations receive higher weights. However, this approach treats fairness separately from learning, potentially missing interactions between fairness and model optimization.

\paragraph{Resampling.}
Bellamy et al.~\cite{bellamy2018ai} propose removing samples from overrepresented group-label combinations (undersampling) or duplicating samples from underrepresented combinations (oversampling). While simple, resampling can reduce dataset size or introduce overfitting.

\paragraph{Fair Representations.}
Zemel et al.~\cite{zemel2013learning} learn intermediate representations that preserve predictive information while obfuscating sensitive attributes. This approach requires training an additional encoder, increasing complexity.

\subsection{In-processing Methods}

In-processing methods incorporate fairness directly into model training.

\paragraph{Fairness Constraints.}
Zafar et al.~\cite{zafar2017fairness,zafar2019fairness} formulate fairness as constraints in the optimization objective:
\begin{equation}
    \min_\theta \mathcal{L}(\theta) \quad \text{s.t.} \quad \text{fairness}(\theta) \leq \epsilon
\end{equation}
where $\mathcal{L}$ is the standard loss and $\epsilon$ bounds the fairness violation. While theoretically elegant, constrained optimization can suffer from convergence issues and computational complexity.

\paragraph{Adversarial Debiasing.}
Zhang et al.~\cite{zhang2018mitigating} train a predictor to maximize accuracy while an adversary tries to predict the sensitive attribute from the predictor's internal representations. The minimax game encourages fair representations:
\begin{equation}
    \min_\theta \max_\phi \mathcal{L}_{\text{pred}}(\theta) - \lambda \mathcal{L}_{\text{adv}}(\phi, \theta)
\end{equation}

Adversarial training requires careful tuning of the $\lambda$ hyperparameter and can be unstable.

\paragraph{Regularization.}
Beutel et al.~\cite{beutel2017data} add fairness penalties to the loss function:
\begin{equation}
    \mathcal{L}_{\text{total}}(\theta) = \mathcal{L}_{\text{pred}}(\theta) + \lambda \cdot \text{fairness penalty}(\theta)
\end{equation}

This approach is simpler than constraints but requires tuning $\lambda$ to balance accuracy and fairness.

\subsection{Post-processing Methods}

Post-processing methods adjust predictions after training to satisfy fairness constraints.

\paragraph{Threshold Optimization.}
Hardt et al.~\cite{hardt2016equality} propose setting group-specific thresholds to satisfy equalized odds:
\begin{equation}
    \hat{y}_i = \begin{cases}
        1 & \text{if } p_i \geq \tau_{z_i} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $\tau_z$ is the threshold for group $z$. While guaranteeing fairness, this introduces inference overhead (per-prediction group lookup and threshold comparison).

\paragraph{Calibrated Equalized Odds.}
Pleiss et al.~\cite{pleiss2017fairness} refine threshold optimization to maintain calibration within groups. However, this does not address calibration degradation caused by the training process itself.

\subsection{Positioning Our Work}

Our adaptive weighting method is an \textbf{in-processing} approach that:
\begin{itemize}
    \item Requires no fairness-specific constraints (simpler than~\cite{zafar2017fairness})
    \item Avoids adversarial training instability (unlike~\cite{zhang2018mitigating})
    \item Introduces \textbf{zero inference overhead} (unlike post-processing~\cite{hardt2016equality})
    \item Achieves perfect fairness (EO=0.0), which prior in-processing methods do not demonstrate on real data
\end{itemize}

\section{Meta-Learning for Fairness}
\label{sec:meta_fairness}

Meta-learning, or ``learning to learn,'' trains models to quickly adapt to new tasks~\cite{finn2017model}. Recent work has explored meta-learning for fairness.

\subsection{Model-Agnostic Meta-Learning (MAML)}

Finn et al.~\cite{finn2017model} introduced MAML, which learns initial parameters $\theta_0$ that enable fast adaptation to new tasks via few gradient steps:
\begin{equation}
    \theta_0 = \arg\min_\theta \sum_{\text{tasks } \tau} \mathcal{L}_\tau(\theta - \alpha \nabla_\theta \mathcal{L}_\tau(\theta))
\end{equation}

MAML has been applied to few-shot learning, reinforcement learning, and domain adaptation.

\subsection{Fair Meta-Learning}

Celis et al.~\cite{celis2020meta} propose meta-learning fair representations that transfer across tasks with different sensitive attributes. The meta-objective encourages representations that are both predictive and fair.

Donini et al.~\cite{donini2018empirical} use multi-task learning to train fair models across multiple datasets, sharing representations while task-specific classifiers enforce fairness constraints.

\subsection{Our Findings on Meta-Learning}

In our Day 15 experiments (hybrid methods), we tested combining meta-learning with adaptive weighting using interpolation parameter $\alpha \in [0, 1]$. Surprisingly, \textbf{pure adaptive weighting ($\alpha=0$) outperformed all hybrid configurations}, with meta-learning providing no benefit. This suggests that for our setting (binary classification with sample weighting), simpler methods suffice. We do not pursue meta-learning further in this thesis.

\section{Calibration in Machine Learning}
\label{sec:calibration}

Calibration refers to the alignment between predicted probabilities and true outcome frequencies. A perfectly calibrated classifier satisfies:
\begin{equation}
    P(Y=1 \mid f(X)=p) = p \quad \forall p \in [0,1]
\end{equation}

\subsection{Measuring Calibration}

\paragraph{Reliability Diagrams.}
Reliability diagrams~\cite{degroot1983comparison} visualize calibration by binning predictions and plotting bin accuracy versus bin confidence. Well-calibrated models produce points near the diagonal.

\paragraph{Expected Calibration Error (ECE).}
ECE~\cite{naeini2015obtaining} quantifies the average deviation from perfect calibration:
\begin{equation}
    \text{ECE} = \sum_{b=1}^B \frac{n_b}{n} |\text{acc}(b) - \text{conf}(b)|
\end{equation}
Lower ECE indicates better calibration.

\paragraph{Brier Score.}
The Brier score~\cite{brier1950verification} measures mean squared error of probabilities:
\begin{equation}
    \text{Brier} = \frac{1}{n} \sum_{i=1}^n (p_i - y_i)^2
\end{equation}
It combines calibration and refinement (discrimination ability).

\subsection{Calibration of Modern Models}

Guo et al.~\cite{guo2017calibration} found that modern neural networks are often miscalibrated, with ECE increasing with model capacity. They attribute this to overfitting and propose temperature scaling:
\begin{equation}
    p^{(T)}_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\end{equation}
where $z_i$ are logits and $T$ is a temperature parameter learned on a validation set.

\subsection{Fairness and Calibration}

HÃ©bert-Johnson et al.~\cite{hebert2018multicalibration} introduce \textbf{multi-calibration}, requiring calibration to hold within intersections of demographic groups:
\begin{equation}
    P(Y=1 \mid f(X)=p, X \in \mathcal{G}) = p \quad \forall \mathcal{G} \subseteq \mathcal{X}
\end{equation}

Pleiss et al.~\cite{pleiss2017fairness} study calibration disparities, showing that satisfying equalized odds can induce different calibration levels across groups when base rates differ.

\subsection{Our Contribution to Calibration}

We are the first to systematically measure how \textbf{in-processing fairness interventions affect calibration}. Prior work assumes fairness-accuracy trade-offs~\cite{menon2018cost}, but we discover that the primary sacrifice is \textbf{calibration}, not accuracy. This finding has significant implications for applications requiring well-calibrated probabilities (e.g., medical diagnosis).

\section{Sample Weighting Methods}
\label{sec:sample_weighting}

Sample weighting assigns importance scores to training examples, emphasizing some samples over others during optimization.

\subsection{Cost-Sensitive Learning}

Elkan~\cite{elkan2001foundations} formalized cost-sensitive learning, where misclassification costs vary by class. Samples from the minority class receive higher weights to correct class imbalance:
\begin{equation}
    w_i = \begin{cases}
        c_{\text{pos}} & \text{if } y_i = 1 \\
        c_{\text{neg}} & \text{if } y_i = 0
    \end{cases}
\end{equation}
where $c_{\text{pos}}, c_{\text{neg}}$ are class-specific costs.

\subsection{Importance Weighting}

Shimodaira~\cite{shimodaira2000improving} introduced importance weighting for covariate shift, where training and test distributions differ. Weights correct the distribution mismatch:
\begin{equation}
    w_i = \frac{P_{\text{test}}(x_i)}{P_{\text{train}}(x_i)}
\end{equation}

This approach requires estimating density ratios, which can be unstable.

\subsection{Boosting}

AdaBoost~\cite{freund1997decision} iteratively reweights samples, increasing weights for misclassified examples:
\begin{equation}
    w_i^{(t+1)} = w_i^{(t)} \exp\left( \alpha^{(t)} \cdot \mathbb{1}[\hat{y}_i^{(t)} \neq y_i] \right)
\end{equation}

This forces subsequent learners to focus on hard-to-classify samples. Interestingly, our approach does the opposite: we upweight confident \emph{correct} predictions.

\subsection{Fairness-Aware Weighting}

Calders and Verwer~\cite{calders2009building} propose fairness-aware weighting, assigning weights based on group membership and label to balance representation. Lahoti et al.~\cite{lahoti2020fairness} use adversarial reweighting, where weights are learned to maximize fairness while maintaining accuracy.

\subsection{Our Weighting Formula}

Our adaptive weighting formula:
\begin{equation}
    w_i = (c_i \times r_i + \epsilon)^{1/T}
\end{equation}
is \textbf{novel} in that it combines:
\begin{itemize}
    \item \textbf{Confidence} ($c_i$): Unlike boosting, which ignores confidence, we upweight high-confidence predictions
    \item \textbf{Correctness} ($r_i$): Unlike cost-sensitive learning, which weights by class, we weight by prediction quality
    \item \textbf{Temperature} ($T$): Controls weight sharpness, analogous to softmax temperature but applied to weights rather than logits
\end{itemize}

Counterintuitively, upweighting confident correct predictions---samples the model already handles well---improves fairness. Our interpretability analysis (Chapter~\ref{ch:results}, \S\ref{sec:interpretability}) explains why: these ``easy'' samples provide stable gradients that reduce disparity without sacrificing accuracy.

\section{Summary and Positioning}
\label{sec:related_summary}

\begin{table}[t]
    \centering
    \caption{Comparison of fairness interventions. Our method achieves perfect fairness with zero inference overhead but degrades calibration.}
    \label{tab:related_comparison}
    \begin{tabular}{@{}lccccc@{}}
        \toprule
        \textbf{Method} & \textbf{Type} & \textbf{Perfect EO?} & \textbf{Inference OH} & \textbf{Calibration} & \textbf{Complexity} \\
        \midrule
        Reweighing~\cite{kamiran2012data} & Pre & No & 0\% & Preserved & Low \\
        Constraints~\cite{zafar2017fairness} & In & No & 0\% & Unknown & High \\
        Adversarial~\cite{zhang2018mitigating} & In & No & 0\% & Unknown & High \\
        Threshold Opt.~\cite{hardt2016equality} & Post & Yes* & +50--200\% & Preserved & Low \\
        \textbf{Ours (Adaptive)} & In & \textbf{Yes} & \textbf{0\%} & \textbf{Degrades} & \textbf{Low} \\
        \bottomrule
    \end{tabular}
    \\[0.5em]
    \footnotesize *Threshold optimization can achieve perfect fairness theoretically but at the cost of inference overhead.
\end{table}

Table~\ref{tab:related_comparison} positions our work relative to prior fairness interventions. Our key differentiators are:

\begin{enumerate}
    \item \textbf{Perfect fairness}: We demonstrate EO=0.0 on real data (German Credit), which prior in-processing methods have not achieved.
    
    \item \textbf{Zero inference overhead}: Unlike post-processing~\cite{hardt2016equality}, we produce standard models deployable without modification.
    
    \item \textbf{Calibration trade-off}: We are the first to systematically quantify how in-processing fairness interventions degrade calibration (+388--756\% ECE).
    
    \item \textbf{Simplicity}: Our method requires 10 lines of code (weight computation + weighted training), compared to complex adversarial training~\cite{zhang2018mitigating} or constrained optimization~\cite{zafar2017fairness}.
\end{enumerate}

The primary limitation is calibration degradation, which we fully characterize in Chapter~\ref{ch:results}. This trade-off is fundamental to our approach, not a tunable hyperparameter artifact.

Having reviewed related work, we now formalize our methodology in Chapter~\ref{ch:methodology}.
